{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e566d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Boilerplate: BART inference on a dataset (CSV/JSON/TSV)\n",
    "\n",
    "Features:\n",
    "- Loads dataset with `datasets` (CSV/JSON/TSV) or plain text file\n",
    "- Loads a BART-family model from Hugging Face (e.g., facebook/bart-large-cnn or facebook/bart-base)\n",
    "- Tokenizes and runs batch generation on GPU if available\n",
    "- Supports generation arguments: max_length, num_beams, do_sample, top_k, top_p, temperature\n",
    "- Writes outputs to CSV with original input + generated text\n",
    "\n",
    "Usage examples:\n",
    "python bart_inference_boilerplate.py \\\n",
    "  --model facebook/bart-large-cnn \\\n",
    "  --input-file input.csv \\\n",
    "  --input-column text \\\n",
    "  --output-file outputs.csv \\\n",
    "  --batch-size 8 \\\n",
    "  --max-length 128 \\\n",
    "  --num-beams 4\n",
    "\n",
    "Requirements:\n",
    "- transformers\n",
    "- datasets\n",
    "- torch\n",
    "\n",
    "Install: pip install transformers datasets torch\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"BART inference boilerplate\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"Hugging Face model name (e.g., facebook/bart-large-cnn)\")\n",
    "    parser.add_argument(\"--input-file\", type=str, required=True, help=\"Path to input dataset (csv/json/tsv or plain txt)\")\n",
    "    parser.add_argument(\"--input-column\", type=str, default=\"text\", help=\"Column name to read source text from for CSV/JSON/TSV datasets\")\n",
    "    parser.add_argument(\"--output-file\", type=str, default=\"outputs.csv\", help=\"CSV file to write results\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=8)\n",
    "    parser.add_argument(\"--max-length\", type=int, default=142)\n",
    "    parser.add_argument(\"--min-length\", type=int, default=56)\n",
    "    parser.add_argument(\"--num-beams\", type=int, default=4)\n",
    "    parser.add_argument(\"--device\", type=str, default=None, help=\"'cpu' or 'cuda' or leave empty to auto-detect\")\n",
    "    parser.add_argument(\"--task\", type=str, default=\"summarization\", choices=[\"summarization\", \"generation\"], help=\"Task type: summarization or generation\")\n",
    "    parser.add_argument(\"--do-sample\", action=\"store_true\", help=\"Enable sampling (stochastic generation)\")\n",
    "    parser.add_argument(\"--top-k\", type=int, default=50)\n",
    "    parser.add_argument(\"--top-p\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--max-input-length\", type=int, default=1024, help=\"Truncate inputs longer than this (tokens)\")\n",
    "    parser.add_argument(\"--progress\", action=\"store_true\", help=\"Show simple progress\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def load_inputs(input_file: str, input_column: str = \"text\") -> List[Dict[str, str]]:\n",
    "    \"\"\"Load inputs into a list of dicts with key 'text' (and keep other columns if present).\n",
    "    Supports: csv, tsv, json, txt (one example per line)\n",
    "    \"\"\"\n",
    "    p = Path(input_file)\n",
    "    suffix = p.suffix.lower()\n",
    "    records = []\n",
    "\n",
    "    if suffix in {\".csv\", \".tsv\"}:\n",
    "        delim = \"\\t\" if suffix == \".tsv\" else \",\"\n",
    "        with open(p, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter=delim)\n",
    "            for row in reader:\n",
    "                text = row.get(input_column)\n",
    "                if text is None:\n",
    "                    raise ValueError(f\"Input column '{input_column}' not found in {input_file}. Columns: {list(row.keys())}\")\n",
    "                row[\"text\"] = text\n",
    "                records.append(row)\n",
    "    elif suffix == \".json\":\n",
    "        # assume newline-delimited json or a list\n",
    "        try:\n",
    "            ds = load_dataset(\"json\", data_files=str(p))\n",
    "            for ex in ds[\"train\"]:\n",
    "                if input_column not in ex:\n",
    "                    raise ValueError(f\"Input column '{input_column}' not found in JSON dataset\")\n",
    "                records.append({**ex, \"text\": ex[input_column]})\n",
    "        except Exception:\n",
    "            # fallback: try line-by-line\n",
    "            import json as _json\n",
    "            with open(p, encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    ex = _json.loads(line)\n",
    "                    if input_column not in ex:\n",
    "                        raise ValueError(f\"Input column '{input_column}' not found in JSON lines file\")\n",
    "                    ex[\"text\"] = ex[input_column]\n",
    "                    records.append(ex)\n",
    "    elif suffix == \".txt\":\n",
    "        with open(p, encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                records.append({\"text\": line.strip(), \"_line\": i})\n",
    "    else:\n",
    "        # try datasets to infer format (works for many common formats)\n",
    "        try:\n",
    "            ds = load_dataset(str(p))\n",
    "            for ex in ds[\"train\"]:\n",
    "                if input_column not in ex:\n",
    "                    raise ValueError(f\"Input column '{input_column}' not found in inferred dataset\")\n",
    "                records.append({**ex, \"text\": ex[input_column]})\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Unsupported file format or failed to load: {e}\")\n",
    "\n",
    "    if not records:\n",
    "        raise ValueError(\"No examples loaded from input file\")\n",
    "    return records\n",
    "\n",
    "\n",
    "def chunked(iterable, size):\n",
    "    for i in range(0, len(iterable), size):\n",
    "        yield iterable[i:i + size]\n",
    "\n",
    "\n",
    "def generate_batch(\n",
    "    texts: List[str],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForSeq2SeqLM,\n",
    "    device: torch.device,\n",
    "    gen_kwargs: Dict,\n",
    "    max_input_length: int = 1024,\n",
    "):\n",
    "    # Tokenize with truncation\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    attention_mask = enc.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "\n",
    "    # decode\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "\n",
    "# device selection\n",
    "if args.device:\n",
    "    device = torch.device(args.device)\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Loading tokenizer and model: {args.model} on device={device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args.model)\n",
    "\n",
    "# move model to device (and optionally fp16 if using CUDA and supported)\n",
    "model.to(device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        model.half()\n",
    "        print(\"Converted model to float16 (half) for faster inference on CUDA\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "records = load_inputs(args.input_file, args.input_column)\n",
    "texts = [r[\"text\"] for r in records]\n",
    "\n",
    "gen_kwargs = {\n",
    "    \"max_length\": args.max_length,\n",
    "    \"min_length\": args.min_length,\n",
    "    \"num_beams\": args.num_beams,\n",
    "    \"do_sample\": args.do_sample,\n",
    "    \"top_k\": args.top_k if args.do_sample else None,\n",
    "    \"top_p\": args.top_p if args.do_sample else None,\n",
    "    \"temperature\": args.temperature if args.do_sample else None,\n",
    "    # allow returning tensors for safety (we'll decode after)\n",
    "}\n",
    "# remove None values\n",
    "gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "batch_size = args.batch_size\n",
    "results = []\n",
    "\n",
    "total = len(texts)\n",
    "for i, batch_idxs in enumerate(chunked(list(range(total)), batch_size)):\n",
    "    batch_texts = [texts[j] for j in batch_idxs]\n",
    "    outputs = generate_batch(batch_texts, tokenizer, model, device, gen_kwargs, args.max_input_length)\n",
    "    for idx, out in zip(batch_idxs, outputs):\n",
    "        rec = {**records[idx]}\n",
    "        rec[\"generated\"] = out\n",
    "        results.append(rec)\n",
    "    if args.progress:\n",
    "        print(f\"Processed {min((i + 1) * batch_size, total)}/{total}\")\n",
    "\n",
    "# write results to CSV (columns: original keys + generated)\n",
    "out_keys = list(results[0].keys())\n",
    "# ensure deterministic column order: put 'text' then 'generated' near end\n",
    "if \"text\" in out_keys:\n",
    "    out_keys = [k for k in out_keys if k != \"text\"]\n",
    "    out_keys.insert(0, \"text\")\n",
    "if \"generated\" in out_keys:\n",
    "    out_keys = [k for k in out_keys if k != \"generated\"]\n",
    "    out_keys.append(\"generated\")\n",
    "\n",
    "with open(args.output_file, \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=out_keys)\n",
    "    writer.writeheader()\n",
    "    for r in results:\n",
    "        writer.writerow({k: (r.get(k, \"\") if r.get(k, \"\") is not None else \"\") for k in out_keys})\n",
    "\n",
    "print(f\"Wrote {len(results)} records to {args.output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
